<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD><TITLE>4 Efficient Implementation</TITLE><LINK href="ozdoc.css" rel="stylesheet" type="text/css"></HEAD><BODY><TABLE align="center" border="0" cellpadding="6" cellspacing="6" class="nav"><TR bgcolor="#DDDDDD"><TD><A href="node3.html#chapter.entities">&lt;&lt; Prev</A></TD><TD><A href="index.html">- Up -</A></TD><TD><A href="node5.html#chapter.failures">Next &gt;&gt;</A></TD></TR></TABLE><DIV id="chapter.efficient"><H1><A name="chapter.efficient">4 Efficient Implementation</A></H1><P>The Distribution Subsystem of Mozart boosts throughput and can communicate with an unbounded number of sites in spite of bounded resources such as memory or a limited number of connections. The ability to distribute any entity does not impose any noticeable performance loss on local entities. These properties are achieved by an efficient implementation which will be discussed in this section. </P><H2><A name="label12">4.1 Throughput</A></H2><P>Over most transport media throughput can be boosted by sending few fairly large packets rather than many small ones. The DS utilizes this by pipelining, that is sending several messages to the same destination in one packet. This is possible since all messages to one site are sent via the same (virtual) connection, and since all sends are asynchronous. Asynchronous sends allow a short delay between the time a message is constructed and actually sent. During this period of time, more messages from the same or a different computation may be constructed and can be sent together. <A href="node4.html#label15"><SUP>1</SUP></A> </P><P>The perceived throughput consists not only of the amount of bytes transmitted, but also of the importance of the transmitted data. Important data is therefore sent with a higher priority. Large low priority messages may also be interleaved by high priority messages by sending large messages in pieces. Note that no messages are lost. </P><H2><A name="label13">4.2 Resource Usage</A></H2><P>The resources that are limited in this context are mainly memory and number of connections. The number of connections available depends on what transfer medium and what operating system is used. For a TCP-connection the limit is imposed by a limited number of file descriptors. </P><DIV class="apropos"><P class="margin">Memory</P><P></P></DIV><P>The largest amount of memory necessary for communication is that of the buffers necessary for marshaling. To avoid a memory blowup when sending large messages, the DS uses a suspendable marshaler. This marshaler marshals data to buffers limited in size and suspends in case they are filled. The partial message can then be sent and marshaling can be continued later. </P><DIV class="apropos"><P class="margin">Connections</P><P></P></DIV><P>The fact that only one virtual connection per pair of sites is used, limits the number of physical connections. Unfortunately, when resources are scarce, this is not always enough. Some applications may acquire references to a large number of sites without ever communicating with all of these. Other applications may need to do concurrent communication with more sites than can be simultaneously physically connected. The former case is handled by the automatic connection establishment property, the latter by a round robin scheduling mechanism that lets all virtual connections take turns on having one of the possible physical connections on a need to use basis. </P><H2><A name="label14">4.3 Zero Impact Implementation of the Distribution Subsystem</A></H2><P>The Distribution Subsystem does not interfere with the performance of the local Mozart engine. This is achieved in several ways: </P><UL><LI><P>The DS is a dynamically loadable library that will only be loaded into the system when communication is needed.</P></LI><LI><P>All sockets used for communication are non-blocking, enabling the local computation to advance while the network is transmitting data.</P></LI><LI><P>The implementation of entities is done in such a way that the distributed version will only be invoked when necessary.</P></LI><LI><P>The execution time of the operating system process is shared between the Mozart Virtual Machine and the DS. This is achieved by passing messages asynchronously via queues between the two systems, and processing them in each systems time-slice (See <A href="node4.html#figure.time-sharing">Figure&nbsp;4.1</A>). </P><DIV id="figure.time-sharing"><HR><P><A name="figure.time-sharing"></A></P><DIV align="center"><IMG alt="" src="time_sharing.gif"></DIV><P class="caption"><STRONG>Figure&nbsp;4.1:</STRONG> Time sharing between the Mozart Virtual Machine and the Distribution Subsystem</P><HR></DIV><P></P></LI></UL><P> </P></DIV><TABLE align="center" border="0" cellpadding="6" cellspacing="6" class="nav"><TR bgcolor="#DDDDDD"><TD><A href="node3.html#chapter.entities">&lt;&lt; Prev</A></TD><TD><A href="index.html">- Up -</A></TD><TD><A href="node5.html#chapter.failures">Next &gt;&gt;</A></TD></TR></TABLE><HR align="left" width="30%"><DIV class="footnote"><A name="label15">1. </A>This is a more fine-grained version of the Nagle algorithm.</DIV><HR><ADDRESS><A href="http://www.sics.se/~erik">Erik&nbsp;Klintskog</A> and&nbsp;<A href="http://www.sics.se/~annan">Anna&nbsp;Neiderud</A><BR><SPAN class="version">Version 1.4.0 (20080702)</SPAN></ADDRESS></BODY></HTML>
